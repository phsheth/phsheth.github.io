---
layout: post
title:  "First Month at Kaggle"
excerpt: "I took part in my first Kaggle competition. Probably a start of my competitive machine learning journey."
date:   2019-09-05 20:30:00 +0530
categories: blog
comments: true
tag: machine learning, competitive, data science
---

I received a (mass) email from [Dan Becker](https://www.kaggle.com/dansbecker) at [Kaggle](https://www.kaggle.com) regarding joining a learners course for competing at Kaggle. I remember Dan from his keras course at DataCamp. The course was good and showed how to take part in a Kaggle competition. After the learner's course - a learner's only competition was organised for everyone to take part.

I had considered working on Kaggle about two years back - but never got around to doing it. This push from Dan made it sort of easy for me to re-hone my machine learning implementation skills. The last ML project I had worked on was the [GE Analytics Certification](https://ph.sheth.cc/blog/2018/12/17/2018_review/) in November last year. Machine Learning as a skill is like a muscle - you don't work on it regularly - it doesn't build.

So, the learner's competition that Kaggle is hosting is [Classify forest types based on information about the area](https://ph.sheth.cc/blog/2018/12/17/2018_review/). The training data they have given is from a dataset provided by Jock A. Blackard and Colorado State University. It is a set of observations taken on 30m x 30m patch of land, which finally points to the type of tree which is the label. Our job is to come up with an accurate machine learning model that will predict the type of tree from the given data.

I started by applying the same strategy I had used for the GE Analytics Certification](https://ph.sheth.cc/blog/2018/12/17/2018_review/) case study. Without going into the problem statement of the case study (because it is GE internal data, and I cannot discuss it here in public domain), here is the general strategy I used:

